
# KUKA iiwa 14 Visual-Servoing

## Overview

This project is developed as part of the Robotics Lab course assignment to implement a vision-based task using ROS (Robot Operating System) and
Gazebo simulation environment. The task involves detecting a circular object using OpenCV, controlling a simulated robot to align with an ArUco marker,
and implementing advanced control algorithms to track and align the robot's camera with a moving object.

## Objectives

The primary objectives of this project are:
1. **Object Detection**: Construct a Gazebo world with a circular object and detect it using the `opencv_ros` package.
2. **Robot Control**: Modify the vision-based control node to align the robot's camera with an ArUco marker using a position and orientation offset.
3. **Advanced Control Algorithms**: Implement and test an improved look-at-point control algorithm and a dynamic version of the vision-based controller.

## Project Structure

The project is organized into several components, each corresponding to different stages of development and testing. The primary components include:

1. **Gazebo World Construction**: 
    - A new model representing a 15 cm radius colored circular object is created and inserted into a Gazebo world.
    - The robot is configured in the Gazebo world to view the object through its camera.

2. **Object Detection with OpenCV**:
    - The `opencv_ros_node` is modified to subscribe to the camera feed, detect the circular object using blob detection, and republish the processed image with the detected object highlighted.

3. **Vision-Based Robot Control**:
    - The `kdl_robot_vision_control` node is modified to implement a vision-based task that aligns the robot's camera with an ArUco marker.
    - The robot's tracking capability is demonstrated by moving the ArUco marker and plotting the velocity commands sent to the robot.

4. **Improved Look-At-Point Algorithm**:
    - An improved look-at-point algorithm is implemented using an S2 task space, mapping linear and angular velocities of the camera to changes in the unit-norm axis.
    - The algorithm is tested to verify that the robot maintains alignment with the marker while executing joint space movements.

5. **Dynamic Vision-Based Control**:
    - A dynamic version of the vision-based controller is developed, tracking reference velocities generated by the look-at-point control law using joint space and Cartesian space inverse dynamics controllers.
    - The results are analyzed in terms of commanded joint torques and Cartesian error norms along the performed trajectories.

## Installation

To set up the project and run the simulation, follow these steps:

1. **Install ROS Noetic**:
    Ensure that ROS Noetic is installed on your system.

2. **Clone the Repository**:
    ```bash
    cd .../catkin_ws/src
    git clone https://github.com/EmanueleCuzzocrea/Homework3.git
    ```

3. **Build the Package**:
    Compile the ROS workspace:
    ```bash
    cd .../catkin_ws
    catkin_make
    source devel/setup.bash
    ```

## Conclusion

This project demonstrates the integration of vision-based control algorithms in a simulated environment using ROS and Gazebo.
The successful detection and tracking of objects, along with the implementation of advanced control algorithms, highlight the potential
for further development and application in real-world robotic systems.
